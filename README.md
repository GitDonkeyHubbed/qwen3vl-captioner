<p align="center">
  <img src="assets/VL_GGUF_Captioner GUI Screenshot 2.png" alt="QWEN 3 VL ABL Captioner" width="900"/>
</p>

<h1 align="center">QWEN 3 VL ABL Captioner V1.2.0 ‚Äî GGUF Engine</h1>
<h3 align="center">Professional GPU-Accelerated Image Captioning for Datasets</h3>

<p align="center">
  <img src="https://img.shields.io/badge/version-1.2.0-blue" alt="Version"/>
  <img src="https://img.shields.io/badge/python-3.12-blue?logo=python" alt="Python"/>
  <img src="https://img.shields.io/badge/GPU-CUDA%2012.x-green?logo=nvidia" alt="CUDA"/>
  <img src="https://img.shields.io/badge/platform-Windows-lightgrey?logo=windows" alt="Platform"/>
</p>

---

## üöÄ What's New in V1.2.0

This release brings a major overhaul to how captions are generated, focusing on **accuracy, anatomy, and detail** over "storytelling" fluff.

### üè• Clinical Precision Mode
We've completely rewritten the prompts for all models (Flux, Stable Diffusion, Pony, etc.). Instead of "cinematic" or "moody" descriptions, the engine now focuses on:
- **Physical Reality**: Exact shapes, textures, and spatial relations.
- **Accurate Anatomy**: Detailed descriptions of bodies and poses without euphemisms.
- **Objective Detail**: "Horses through" the image content, listing exactly what is there.

### üîû Uncensored / Adult Detail Option
A new **"Uncensored / Adult Detail"** checkbox in the settings.
When enabled, this injects explicit instructions to describe **all** content (including nudity and adult themes) with full anatomical accuracy, bypassing standard safety refusals. Essential for high-quality dataset training.

---

## ‚ú® Key Features

- **Clinical Precision**: Using anatomically accurate, objective language instead of "creative writing" style. Designed for training, not storytelling.
- **Universal "Edit" Mode**: Full control via the Edit button to handle any prompt format (JSON, XML, Booru) without needing complex hardcoded "modes".
- **Lean Architecture**: Focused on speed and simplicity. No bloat, just tools that work.
- **Multi-Model Presets**: Pre-configured formats for **Flux 1 & 2**, **Stable Diffusion**, **Pony (SDXL)**, **Z-Image**, and more.
- **Drag & Drop**: Drop images or entire folders directly into the app.
- **Batch Processing**: Caption thousands of images automatically.
- **Smart Model Handling**: Native GGUF support with auto-downloading.
- **Hardware Monitoring**: Real-time GPU VRAM usage display.
- **Safety Controls**: Toggle between "PG" and fully "Uncensored" modes.

---

## üì∏ Screenshots

<p align="center">
  <img src="assets/screenshot.png" alt="Main Interface" width="800"/>
  <br/>
  <em>Main workspace: file browser, image viewer, caption editor, and model settings</em>
</p>

---

## ‚ö° Quick Start

### 1. Run Setup
Double-click `setup.bat` to automatically install Python and all necessary dependencies.

### 2. Get Models
You can download models directly inside the app, or place your `.gguf` files in this same folder.
**Recommended:** `Qwen3-VL-8B-Instruct-abliterated-v1.Q6_K.gguf`

### 3. Launch
Double-click `run.bat` to start the captioner.

---

## üìÅ Project Structure

```
qwen3vl-captioner/
‚îú‚îÄ‚îÄ app.py                  # Application entry point
‚îú‚îÄ‚îÄ run.bat                 # Launch script (Windows)
‚îú‚îÄ‚îÄ setup.bat               # Automated installer (Windows)
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ pyproject.toml          # Project metadata
‚îÇ
‚îú‚îÄ‚îÄ engine/                 # Inference backend
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ inference.py        # Qwen3VLEngine ‚Äî GGUF model loading & captioning
‚îÇ   ‚îî‚îÄ‚îÄ model_downloader.py # HuggingFace model download manager
‚îÇ
‚îú‚îÄ‚îÄ gui/                    # PyQt6 user interface
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main_window.py      # Main window orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ settings_panel.py   # Right panel ‚Äî presets, parameters, batch controls
‚îÇ   ‚îú‚îÄ‚îÄ file_browser.py     # Left sidebar ‚Äî thumbnail list with search
‚îÇ   ‚îú‚îÄ‚îÄ image_viewer.py     # Center ‚Äî image preview with zoom controls
‚îÇ   ‚îú‚îÄ‚îÄ caption_panel.py    # Bottom ‚Äî caption display, edit, and save
‚îÇ   ‚îú‚îÄ‚îÄ dataset_panel.py    # Dataset table view
‚îÇ   ‚îú‚îÄ‚îÄ theme.py            # Dark/light theme color system & QSS stylesheet
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # User config persistence (~/.vlcaptioner/)
‚îÇ   ‚îú‚îÄ‚îÄ notification_panel.py
‚îÇ   ‚îú‚îÄ‚îÄ app_settings_dialog.py
‚îÇ   ‚îî‚îÄ‚îÄ model_download_manager.py
‚îÇ
‚îî‚îÄ‚îÄ assets/
    ‚îî‚îÄ‚îÄ screenshot.png      # GUI screenshot for README
```

---

## üîß How It Works

### Engine
The app uses [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) (JamePeng's fork with Qwen3-VL support) to run **GGUF quantized** models directly on your GPU via CUDA. No cloud API, no internet required after model download.

- **Model:** Qwen3-VL 8B Instruct (abliterated variant for uncensored captioning)
- **Quantization:** Q6_K (~6.3 GB) or Q8_0 (~8.1 GB) ‚Äî excellent quality-to-size ratio
- **Vision encoder:** Separate mmproj file handles image understanding
- **Inference:** GPU-accelerated with streaming token output

### Captioning Workflow

1. **Open Folder** ‚Üí Select your dataset directory (all images load instantly)
2. **Load Model** ‚Üí One-click model loading with CUDA auto-detection
3. **Configure** ‚Üí Choose a target preset (SD, Flux, etc.), adjust length and temperature
4. **Caption** ‚Üí Click individual images + "Regenerate Caption", or "Batch Caption All" for the entire dataset
5. **Export** ‚Üí Save all captions as `.txt` sidecar files next to the originals

### Target Presets

| Preset | Use Case |
|--------|----------|
| **Stable Diffusion** | Comma-separated booru-style tags for SD 1.5 / SDXL training |
| **Flux 1** | Natural language descriptions optimized for Black Forest Labs Flux.1 |
| **Flux 2** | Updated format for Flux.2 model training |
| **Z-Image** | Structured captions for Z-Image architecture |
| **Chroma** | Scene descriptions for Chroma model fine-tuning |
| **Pony (SDXL)** | Pony Diffusion V6 tag format with quality markers |
| **Qwen Image** | General-purpose detailed image descriptions |

---

## ‚å®Ô∏è Manual Installation (Advanced)

If `setup.bat` doesn't work or you prefer manual setup:

```bash
# 1. Create virtual environment
python -m venv .venv
.venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Install llama-cpp-python with CUDA support
# Download the appropriate wheel from:
# https://github.com/JamePeng/llama-cpp-python/releases
pip install llama_cpp_python-0.3.24+cu124.basic-cp312-cp312-win_amd64.whl

# 4. Run
python app.py
```

### Linux / macOS (Experimental)

The GUI is cross-platform (PyQt6) but the setup scripts are Windows-only. For other platforms:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# For CUDA on Linux:
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal on macOS:
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

python app.py
```

> **Note:** Linux/macOS support is experimental. The CUDA DLL preloading in `engine/inference.py` is Windows-specific but will be safely skipped on other platforms.

---

## üõ†Ô∏è System Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **OS** | Windows 10 64-bit | Windows 11 |
| **GPU** | NVIDIA GTX 1070 (8 GB) | NVIDIA RTX 3060+ (12 GB) |
| **VRAM** | 8 GB | 12+ GB |
| **RAM** | 16 GB | 32 GB |
| **Storage** | ~10 GB (model + app) | ~15 GB (both quants) |
| **CUDA** | 12.0 | 12.4 |
| **Python** | 3.11 | 3.12 |

---

## üìù Caption Output Format

Captions are saved as plain `.txt` files with the same name as the image:

```
my_image.jpg     ‚Üí  my_image.txt
photo_001.png    ‚Üí  photo_001.txt
```

This is the standard sidecar format expected by most training tools (Kohya, EveryDream, SimpleTuner, etc.).

---

## üêõ Troubleshooting

| Issue | Solution |
|-------|----------|
| **"Model not found"** | Place `.gguf` files in the parent directory (one level above `qwen3vl-captioner/`) |
| **"CUDA not available"** | Install [CUDA Toolkit 12.x](https://developer.nvidia.com/cuda-downloads) and restart |
| **Blank image preview** | Fixed in v1.4.2 ‚Äî Qt image allocation limit raised to handle large files |
| **Slow model loading** | Normal ‚Äî first load takes 30-60s. Subsequent loads are faster |
| **Out of VRAM** | Use Q6_K instead of Q8_0, or reduce `max_tokens` |
| **"access violation"** | CUDA DLLs not found. Run `run.bat` (sets PATH automatically) |

---

## ü§ù Credits

- **[Qwen3-VL](https://huggingface.co/Qwen)** ‚Äî Vision-language model by Alibaba DAMO Academy
- **[llama-cpp-python](https://github.com/abetlen/llama-cpp-python)** ‚Äî Python bindings for llama.cpp
- **[JamePeng's fork](https://github.com/JamePeng/llama-cpp-python)** ‚Äî Added Qwen3-VL chat handler support
- **[prithi's GGUF quants](https://huggingface.co/prithivMLmods/Qwen3-VL-8B-Instruct-abliterated-v1-GGUF)** ‚Äî High-quality GGUF model quantizations
- **[PyQt6](https://www.riverbankcomputing.com/software/pyqt/)** ‚Äî Cross-platform GUI framework

---

## üìÑ License

[MIT License](LICENSE) ‚Äî free to use, modify, and distribute.

---

<p align="center">
  <strong>Made with ‚ù§Ô∏è for the AI art community</strong>
  <br/>
  <em>If this tool helps your workflow, consider giving it a ‚≠ê!</em>
</p>
